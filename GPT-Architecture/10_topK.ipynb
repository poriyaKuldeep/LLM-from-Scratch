{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without TOP-K sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def generatenextword(model,idx,max_token_size,context_Size):\n",
    "\n",
    "    for _ in range(max_token_size):\n",
    "        idx_split=idx[:,-context_Size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit=model(idx_split)\n",
    "\n",
    "        ## logit [ batch tokens vocabsize]\n",
    "        logitslast=logit[:,-1,:]\n",
    "\n",
    "        prob=torch.softmax(logitslast,dim=-1)\n",
    "        maxi=torch.argmax(prob,dim=-1,keepdim=True)\n",
    "        idx=torch.cat((idx,maxi),dim=1)\n",
    "    return idx  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With TOP-K sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n",
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      "tensor([7])\n"
     ]
    }
   ],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging TOP-K with TEMP SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generatenextword(model, idx, max_token_size, context_Size, top_k=5, temperature=1.0):\n",
    "    for _ in range(max_token_size):\n",
    "        idx_split = idx[:, -context_Size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_split)\n",
    "\n",
    "        logits_last = logits[:, -1, :]\n",
    "        \n",
    "        logits_last = logits_last / temperature\n",
    "        \n",
    "       \n",
    "        values, indices = torch.topk(logits_last, k=top_k, dim=-1)\n",
    "        probs = torch.softmax(values, dim=-1)\n",
    "        sampled_idx = torch.multinomial(probs, num_samples=1)\n",
    "        maxi = torch.gather(indices, -1, sampled_idx)\n",
    "       \n",
    "    \n",
    "        idx = torch.cat((idx, maxi), dim=1)\n",
    "    \n",
    "    return idx\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
