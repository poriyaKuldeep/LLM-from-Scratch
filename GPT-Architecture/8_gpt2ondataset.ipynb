{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"verdictbook.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    booktext=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "class DatasetclassV1(Dataset):\n",
    "    def __init__(self,txt,encoder,strides,maxlen):\n",
    "        self.inputlist=[]\n",
    "        self.targetlist=[]\n",
    "\n",
    "        tokenized = encoder.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "    \n",
    "        for i in range(0,len(tokenized)-maxlen,strides):\n",
    "            input_chunk=tokenized[i:i+maxlen]\n",
    "            target_chunk=tokenized[i+1:i+maxlen+1]\n",
    "            self.inputlist.append(torch.tensor(input_chunk))\n",
    "            self.targetlist.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targetlist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputlist[index],self.targetlist[index]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def create_dataloader_v1 (txt, batch_size=4, max_length=256,stride=128, shuffle=True, drop_last=True,num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    datasetobj=DatasetclassV1(txt,tokenizer,stride,max_length)\n",
    "    dataloader = DataLoader(\n",
    "        datasetobj,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(booktext))\n",
    "train_data = booktext[:split_idx]\n",
    "val_data = booktext[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class multiheadv2(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,attention_head,boolbias):\n",
    "        super().__init__()\n",
    "        self.head_dim=d_out//attention_head\n",
    "        self.d_out=d_out\n",
    "        self.attention_head=attention_head\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=boolbias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=boolbias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=boolbias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        b,num_token,d_out=x.shape\n",
    "\n",
    "        keys = self.W_key(x) \n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys=keys.view(b,num_token,self.attention_head,self.head_dim)\n",
    "        queries=queries.view(b,num_token,self.attention_head,self.head_dim)\n",
    "        values=values.view(b,num_token,self.attention_head,self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_score=queries @ keys.transpose(2,3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_token, :num_token]\n",
    "        attn_score.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_score / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        context_vec = context_vec.contiguous().view(b, num_token, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale_params=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift_params=nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return norm*self.scale_params+ self.shift_params\n",
    "    \n",
    "        \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) *(x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "    \n",
    "\n",
    "class feedforward(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(config['emb_dim'],config['emb_dim']*4),\n",
    "            GELU(),\n",
    "            nn.Linear(config['emb_dim']*4,config['emb_dim']),\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__ (self,config):\n",
    "        super().__init__()\n",
    "        self.attn=multiheadv2(d_in=config['emb_dim'],d_out=config['emb_dim'],context_length=config['context_length'],dropout=config['drop_rate'],attention_head=config['n_heads'],boolbias=config['qkv_bias'])\n",
    "        self.Layernorm1=LayerNorm(emb_dim=config['emb_dim'])\n",
    "        self.Layernorm2=LayerNorm(emb_dim=config['emb_dim'])\n",
    "        self.feedforw=feedforward(config=config)\n",
    "        self.dropout=nn.Dropout(config['drop_rate'])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        ## attnetion block\n",
    "        skip=x\n",
    "        x=self.Layernorm1(x)\n",
    "        x=self.attn(x)\n",
    "        x=self.dropout(x)\n",
    "        x=x+skip\n",
    "\n",
    "        ## feed forward nn block\n",
    "        skip=x\n",
    "        x=self.Layernorm2(x)\n",
    "        x=self.feedforw(x)\n",
    "        x=self.dropout(x)\n",
    "        x=x+skip\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_2(nn.Module):\n",
    "    def __init__ (self,cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb=nn.Embedding(cfg['vocab_size'],cfg[\"emb_dim\"])\n",
    "        self.pos_emb=nn.Embedding(cfg['context_length'],cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self,inputidx):\n",
    "        batch_size,seq=inputidx.shape\n",
    "        tokens=self.token_emb(inputidx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq, device=inputidx.device))\n",
    "        x=tokens+pos_embeds\n",
    "        x=self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPT_2(GPT_CONFIG_124M)\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n",
      "Training loss: 10.98758316040039\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) \n",
    "\n",
    "\n",
    "torch.manual_seed(123) \n",
    "\n",
    "with torch.no_grad(): \n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding the entire LLM Pre-training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "   \n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) \n",
    "        \n",
    "      \n",
    "        logits = logits[:, -1, :]  \n",
    "        probas = torch.softmax(logits, dim=-1)  \n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  \n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  \n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) \n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  \n",
    "    model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() \n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() \n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            tokens_seen += input_batch.numel() \n",
    "            global_step += 1\n",
    "\n",
    "\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.945, Val loss 10.048\n",
      "Ep 1 (Step 000005): Train loss 8.189, Val loss 8.466\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 2 (Step 000010): Train loss 6.659, Val loss 7.100\n",
      "Ep 2 (Step 000015): Train loss 6.050, Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and,, and,, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Ep 3 (Step 000020): Train loss 5.762, Val loss 6.516\n",
      "Ep 3 (Step 000025): Train loss 5.408, Val loss 6.430\n",
      "Every effort moves you, and I had been the                                            \n",
      "Ep 4 (Step 000030): Train loss 5.248, Val loss 6.458\n",
      "Ep 4 (Step 000035): Train loss 4.821, Val loss 6.363\n",
      "Every effort moves you, and, and a was, and, and, and a Mrs.       \"I, and to me, and, and the of the of the of the _, the of the _, and he was,\n",
      "Ep 5 (Step 000040): Train loss 4.227, Val loss 6.236\n",
      "Every effort moves you know the picture.  \"Oh, with a little a little of his \" it--and it--and it's it. \"Oh, and I had been the picture--and it, and I had been the picture.  \n",
      "Ep 6 (Step 000045): Train loss 3.955, Val loss 6.236\n",
      "Ep 6 (Step 000050): Train loss 3.423, Val loss 6.190\n",
      "Every effort moves you know the, and I was a--I he had been I had been I had been--his, I felt, I had been to the fact, I had been his own him.            \n",
      "Ep 7 (Step 000055): Train loss 3.279, Val loss 6.209\n",
      "Ep 7 (Step 000060): Train loss 2.552, Val loss 6.166\n",
      "Every effort moves you know the end one of the picture--I looked, and I felt in a little: \"                               \n",
      "Ep 8 (Step 000065): Train loss 2.107, Val loss 6.193\n",
      "Ep 8 (Step 000070): Train loss 1.753, Val loss 6.222\n",
      "Every effort moves you?\"     \"Oh, he said-chairs me in a and silver of the's an awful simpleton, and Mrs.                     \n",
      "Ep 9 (Step 000075): Train loss 1.394, Val loss 6.233\n",
      "Ep 9 (Step 000080): Train loss 1.074, Val loss 6.282\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony.   \"I was no great me!\"  \"Oh, in the moment--as Jack himself at my elbow and as his pictures with a--because he didn't want\n",
      "Ep 10 (Step 000085): Train loss 0.805, Val loss 6.365\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"         He placed them at my elbow and as I turned, and down the room, when I\n",
      "Ep 11 (Step 000090): Train loss 0.590, Val loss 6.465\n",
      "Ep 11 (Step 000095): Train loss 0.471, Val loss 6.429\n",
      "Every effort moves you?\" \"I that my hostess was \"interesting\": on that point I could have given Miss Croft the fullest reassurance. \"Oh, his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Ep 12 (Step 000100): Train loss 0.350, Val loss 6.529\n",
      "Ep 12 (Step 000105): Train loss 0.243, Val loss 6.586\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 13 (Step 000110): Train loss 0.201, Val loss 6.676\n",
      "Ep 13 (Step 000115): Train loss 0.185, Val loss 6.817\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 14 (Step 000120): Train loss 0.136, Val loss 6.781\n",
      "Ep 14 (Step 000125): Train loss 0.145, Val loss 6.811\n",
      "Every effort moves you?\" \"I-stream stroke. . . . I looked up I could have given Miss Croft the fullest reassurance. Gisburn drew back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 15 (Step 000130): Train loss 0.099, Val loss 6.811\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 16 (Step 000135): Train loss 0.105, Val loss 6.926\n",
      "Ep 16 (Step 000140): Train loss 0.072, Val loss 6.959\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 17 (Step 000145): Train loss 0.070, Val loss 7.009\n",
      "Ep 17 (Step 000150): Train loss 0.065, Val loss 6.960\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 18 (Step 000155): Train loss 0.050, Val loss 7.081\n",
      "Ep 18 (Step 000160): Train loss 0.043, Val loss 7.087\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 19 (Step 000165): Train loss 0.043, Val loss 7.132\n",
      "Ep 19 (Step 000170): Train loss 0.033, Val loss 7.101\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 20 (Step 000175): Train loss 0.033, Val loss 7.151\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Training completed in 3.49 minutes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPT_2(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
